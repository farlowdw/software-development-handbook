import ChipDivider from '@site/src/components/ChipDivider';

```python
class Solution:
    def maxProbability(self, n: int, edges: List[List[int]], succProb: List[float], start_node: int, end_node: int) -> float:
        def build_graph(edge_list):
            adj_list = defaultdict(list)
            for i in range(len(edge_list)):
                start, end = edge_list[i]
                edge_weight = succProb[i]
                adj_list[start].append((end, edge_weight))
                adj_list[end].append((start, edge_weight))
                
            return adj_list
        
        graph = build_graph(edges)
        max_prob = [0.0] * n
        max_prob[start_node] = 1.0
        max_heap = [(-1.0, start_node)]
        
        while max_heap:
            curr_prob, node = heapq.heappop(max_heap)
            curr_prob = -curr_prob
            if node == end_node:
                return curr_prob
            
            if curr_prob < max_prob[node]:
                continue
                
            for neighbor, prob in graph[node]:
                new_prob = curr_prob * prob
                if new_prob > max_prob[neighbor]:
                    max_prob[neighbor] = new_prob
                    heapq.heappush(max_heap, (-new_prob, neighbor))
                    
        return 0
```

Since Python only has a min heap, we simulate a max heap by pushing the negated version of whatever current path probability exists to the node in question onto the min heap. We simply need to be careful when handling the sign of the probability (i.e., when popping from and adding to the heap).


<ChipDivider>Using logarithms for the probabilities</ChipDivider> 

```python
class Solution:
    def maxProbability(self, n: int, edges: List[List[int]], succProb: List[float], start_node: int, end_node: int) -> float:
        def build_graph(edge_list):
            adj_list = defaultdict(list)
            for i in range(len(edge_list)):
                start, end = edge_list[i]
                edge_weight = -math.log(succProb[i])
                adj_list[start].append((end, edge_weight))
                adj_list[end].append((start, edge_weight))
                
            return adj_list
        
        max_prob = [float('inf')] * n
        max_prob[start_node] = 0
        graph = build_graph(edges)
        
        max_heap = [(0, start_node)]
        while max_heap:
            curr_prob, node = heapq.heappop(max_heap)
            
            if curr_prob > max_prob[node]:
                continue
            
            for nei in graph[node]:
                nei_node, path_prob = nei
                new_prob = curr_prob + path_prob
                
                if new_prob < max_prob[nei_node]:
                    max_prob[nei_node] = new_prob
                    heapq.heappush(max_heap, (new_prob, nei_node))
        
        return math.exp(-max_prob[end_node])
```

The solution above uses logarithms to increase precision. It is largely based on observations highlighted in [this solution comment](https://leetcode.com/problems/path-with-maximum-probability/solution/1947323):

> I am surprised to see such a long editorial (three different algorithms with a lot of what sounds like chat GPT generated explanations of the code and a plethora of diagrams) that at no point attempts to justify why using these algorithms give the correct answer for the problem.
>
> "We need to find the path from start to end that has the largest product of its edges" - fine
>
> "BFS is an algorithm I know but it's not used on graphs with weighted edges, let's use Dijkstra instead because that one works on weighted graphs" - cool story bro, but Dijkstra is a shortest path algorithm, not a largest product algorithm...
>
> For anyone who is curious why it works and hasn't worked it out themselves:
>
> - Call product of the edges in a path $P = p_1 \cdot\ldots\cdot p_k$
> - We want to find the path with maximum $P$.
> - Since the logarithm is a monotonically growing function, the path with largest $P$ is also the path with largest $\log P$, (and the smallest $-\log P$)
> - Due to the properties of the logarithm, $\log P = \log(p_1 \cdot\ldots\cdot p_k) = \log p_1 + \cdots + \log p_k$
> - Negating both sides gives us the following: $-\log P = -\ log p_1 + (-\log p_2) + \cdots + (-\log p_k)$.
> - In summary: maximizing $P$, the explicit goal of the problem, is equivalent to minimizing $-\log P$, which is just the sum of the negative logarithms of the edge weights. This equivalent modified problem IS a shortest path problem.
> - Furthermore, since $0 \leq p_i \leq 1$, that means $\log p_i \leq 0$ and $-\log p_i \geq 0$. Non-negative edges, a requirement for Dijkstra to work properly.
> - So yeah, TL;DR: "weighted graph, can't be BFS, let's use Dijkstra" is dumb, but since edge weights are probabilities, finding the maximum product is equivalent to a shortest path problem with non-negative edge weights, and Dijkstra "just works"

ChatGPT sheds some light on why we might want to use logarithms for certain problems:

- **Problem:** When calculating the product of many terms, especially in probabilities (e.g., in Bayesian inference or machine learning), the product can become extremely small, leading to underflow.
- **Solution:** Take the logarithm of each term, sum these logarithms, and then exponentiate the result. This approach is particularly useful in algorithms like the Viterbi algorithm or when dealing with partition functions in statistical mechanics.

The equality we take advantage of is as follows:

$$
\log\biggl(\prod_{i=1}^n a_i\biggr) = \sum_{i=1}^n \log(a_i)
$$
