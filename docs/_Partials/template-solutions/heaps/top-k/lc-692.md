```python
class Pair:
    def __init__(self, word, freq):
        self.word = word
        self.freq = freq
        
    def __lt__(self, other):
        return self.freq < other.freq or (self.freq == other.freq and self.word > other.word)

class Solution:
    def topKFrequent(self, words: List[str], k: int) -> List[str]:
        freqs = defaultdict(int)
        for word in words:
            freqs[word] += 1
        
        min_heap = []
        for word, freq in freqs.items():
            heapq.heappush(min_heap, Pair(word, freq))
            if len(min_heap) > k:
                heapq.heappop(min_heap)
                
        res = [''] * k
        i = k - 1
        while min_heap:
            res[i] = heapq.heappop(min_heap).word
            i -= 1
            
        return res
```

Probably the easiest semi-effective heap solution is to actually use a max heap in a somewhat bizarre way where we *do not limit the heap size* and we simply pop `k` elements from the heap &#8212; the `k` elements will have maximal frequency and words with equal frequencies will be popped in such a way that the lexicographically smaller ones remain (because using a max heap in Python means we simulate its usage with a min heap):

```python
class Solution:
    def topKFrequent(self, words: List[str], k: int) -> List[str]:
        freqs = defaultdict(int)
        for word in words:
            freqs[word] += 1
        
        max_heap = []
        for word, freq in freqs.items():
            heapq.heappush(max_heap, (-freq, word))
            
        return [ heapq.heappop(max_heap)[1] for _ in range(k) ]
```

But we should be able to do better than this. The issue is clear: The hardest part of this problem from the standpoint of a heap-based solution is that the first priority is *maximal* frequency, but the second priority is *minimal* lexicographic value of the word whose frequency is being considered. The "top `k`" nature of the problem where we're trying to find words with maximal frequencies indicates we should use a min heap. But complications arise when we run into two words of the same frequency that maybe start with the same first few letters. 

For example, suppose `(3, 'hope')` and `(3, 'home')` are both in the running for the final spot in the list of words we should return (i.e., other words occur less frequently). If we're using a min heap to store the tuples above, then `(3, 'home')` will be popped before `(3, 'hope')` because `3 == 3` and `'home' < 'hope'`; that is, the frequency values are the same, but `'home'` is lexicographically smaller than `'hope'`. But eliminating `'home'` is not what we want in this case! How can we ensure lexicographically *larger* words are considered to be less than lexicographically *smaller* words when the frequencies of the words are the same? 

As the solution above indicates, a savvy approach is to create a user-defined class, `Pair`, where each class instance is an object with the priority information encoded, namely the word itself as well as its frequency. As [the Python docs note](https://docs.python.org/3/reference/datamodel.html#object.__lt__), the expression `x < y` results in calling `x.__lt__(y)`, which has the following signature: `object.__lt__(self, other)`. Hence, our `Pair` class should have `__lt__` defined in such a way that `Pair_1` is considered to be less than `Pair_2` if the word frequency of `Pair_1` is less than the word frequency of `Pair_2` *or* if their word frequencies are the same and the word in `Pair_1` is lexicographically larger than the word in `Pair_2`:

```python
def __lt__(self, other):
    return self.freq < other.freq or (self.freq == other.freq and self.word > other.word)
```

Defining the `Pair` class means we can ensure our heap behaves as desired, and it also paves a path forward for other problems that may need a heap-based solution where we need to get creative in how priorities are managed.