import BibRef from '@site/src/components/BibRef';

```python
class Solution:
    def minCostClimbingStairs(self, cost: List[int]) -> int:
        def dp(step):
            if step in memo:
                return memo[step]
            
            step_cost = min(dp(step - 1) + cost[step - 1], dp(step - 2) + cost[step - 2])
            memo[step] = step_cost
            
            return step_cost
        
        memo = dict()
        memo[0] = memo[1] = 0
        return dp(len(cost)) 
```

---

The solution above may be the simplest, but there are multiple ways of going about this problem. What follows is mostly meant for potential use in the future.

This problem highlights how we can interpret the same problem in two fundamentally different ways and still end up with a correct answer. Specifically, we may interpret the line

> *You can either start from the step with index `0`, or the step with index `1`.*

from the problem stem in two notable ways:

1. **We start *after* step `0`**, and we have to reach step `n` (i.e., we consider the top of the floor to be the step beyond the last step, `n - 1`), where the cost of each step is taken into account once we have departed from that step. In this sense, step `0` and step `1` both cost `0` because we're told we can *start* from the step with index `0` or the step with index `1` &#8212; it costs nothing *to get to that step*, and the cost of that step is only considered once we've left it.
2. **We start *before* step `0`**, and we have to reach the last step, step `n - 1`, where the cost of each step is taken into account once landed on. In this sense, choosing to go to step `0` at the beginning means it costs `cost[0]` to do so; similarly, choosing to go to step `1` instead means it costs `cost[1]` to do so. The goal, then, is to minimize the cost it takes to get to either step `n - 2` or `n - 1` because once we get to either of those steps and that step's cost is taken into account, we can reach the top of the floor, as desired.

Which interpretation we choose to go with ultimately does not matter in terms of the correctness of our result, but the differences will manifest themselves in our implementation(s).

Generally speaking, a DP solution may be implemented top-down with memoization or bottom-up with tabulation. Furthermore, the recurrence used to characterize how subproblems are broken down into smaller and smaller subproblems may be either a *backward* recurrence (i.e., the usual approach where index values *decrease*) or a *forward* recurrence (i.e., more of a *chronological* approach, where index values *increase*).

<details>
<summary> Differences between forward and backward DP</summary>

The differences between forward and backward DP are remarked on elegantly in [a comment on Codeforces](https://codeforces.com/blog/entry/44356?#comment-289127) and reproduced below for ease of reference (the remarks below are in response to the question, "Why do top guys favor array-based versus recursive DP?"):

> I'd say there are several reasons.
> 
> **Tradition**
> 
> People are taught to implement dynamic programming using loops. Then they teach the next generation to implement dynamic programming using loops. Hardly surprising, regardless of whether they are right or not.
> 
> **Convenience**
> 
> To write loops instead of memoization, the required additional effort is to topologically sort the states in a loop-friendly way. This effort often pays off.
> 
> To begin with, the asymptotic complexity of a bunch of for loops is usually much more obvious than the complexity of a recursive solution. Furthermore, many advanced dynamic programming problems will require additional insights after figuring out the most obvious dynamic programming solution. There are often multiple ways to topologically sort the states, and some of them make optimizations possible — and visible. Some examples are computing prefix sums to speed things up (already mentioned a few times), or even getting rid of one of the parameters.
>
> **Forward and Backward Dynamic Programming**
> 
> Often, we can express the objective function $f(s)$ on state $s$ as some combination of $f(t_1), f(t_2), \ldots, f(t_k)$, much like a mathematical formula. This can be expressed as backward dynamic programming. At a certain point in time, we want to calculate $f(s)$. Alright, take $f(t_1), f(t_2), \ldots, f(t_k)$ &#8212; either they are already calculated with a loop-based solution, or we use recursion with memoization to calculate them as needed &#8212; and combine them in the required way (calculate their sum, maximum, or whatever). Here, we move from state $s$ back to states $t_i$ as needed.
> 
> However, some solutions are better expressed as forward dynamic programming. At a certain point in time, consider state $s$. At this point, $f(s)$ is already calculated. We know that some states $r_1, r_2, \ldots, r_p$ depend on state $s$. So, right now, we want to update the intermediate calculations in these states using $f(s)$. When we later arrive at, say, $r_1$, it happens only when we already processed all states it depends on, and thus $f(r_1)$ is already calculated. One example is Dijkstra's algorithm: whenever we add a vertex $s$ to the tree of shortest paths, we update the minimum distance to each of the vertices $r_1, r_2, \ldots, r_p$ directly reachable from $s$.
> 
> With this approach, we simply cannot write the mathematical formula like $f(s) = G(f(t_1), \ldots)$ because the dependencies go the other way! Technically, it is still always possible to transform forward DP into backward DP (the entire graph between the states of dynamic programming can be stored, topologically sorted, and visited in the resulting order), but for some problems, forward DP can be more natural and more suitable for optimizations.

A [response to this comment](https://codeforces.com/blog/entry/44356?#comment-289143) is also informative (paraphrased below):

> I always use forward DP if it's possible. It's similar to BFS and therefore is more natural. It should be easier to think like "OK, I'm standing at `dp[i][j]`, what can I do now?" I think learning DP must start with this approach. Sometimes forward DP is less convenient, e.g. if DP is combined with prefix sums or two pointers or some other things, but when you face such kind of DP, you usually have enough skill to understand all three techniques (forward, backward, recursion) clearly.

There's a decent amount to unpack from these comments, but perhaps the most important piece is the following: *for some problems, forward DP can be more natural and more suitable for optimizations*; that is, it's not as if forward or backward DP should always be used. Use the approach most suitable for solving the problem at hand.

A couple other points from the comments above worth remarking on:

- **Multiple ways to topologically sort states:** "Topologically sorting states" in DP means arranging the order in which we compute each DP state so that all dependencies of a state are computed before the state itself. For some problems, specifically those with multiple dimensions (i.e., state variables) and/or constraints, there will likely be several valid ways to order the states, which means we may compute states in slightly different orders so long as the required prerequisites are always computed first (i.e., whatever ordering of the states we come up with still needs to be a *topological* ordering of the states, where there's freedom in which topological ordering we choose because, in general, topological orderings are not unique).

    This matters because different orders (i.e., different topological sorts of the states) can make some optimizations more visible or easier to implement than others (e.g., using prefix sums or certain optimizations may require computing states row-by-row or column-by-column in a 2D DP array). 
- **The objective function $f(s)$:** The "objective function" remarked on in the comments simply represents the value or result of the DP solution for a particular state $s$; that is, each state $s$ in a DP algorithm corresponds to a subproblem, and $f(s)$ is the *solution* to that subproblem.
- **Dijkstra's algorithm as an example of forward DP:** It may be interesting to note that Dijkstra's algorithm can be viewed as an example of forward DP even though it's often not mentioned in the context of DP. Why can it be viewed as an example of forward DP? Because we start at the source node with a known distance of `0` and then "push" updates to neighboring nodes using the current known distances. This approach closely matches the idea of forward DP: we use already computed values (e.g., `dp[source]`) to update the states that depend on them (e.g., `dp[neighbor]`). It's like saying, "I've computed `dp[some_state]`, and now I will use this to update the states that depend on `dp[some_state]`."
- **Forward DP is similar to BFS:** DPV <BibRef id='DPV' pages='pp. 108-113'></BibRef> introduces Dijkstra's algorithm as an attempt to *extend* BFS beyond just considering unit edge weights. If Dijkstra is an example of forward DP, then how might BFS be similar to forward DP? BFS begins at a starting point and moves *forward* by exploring neighboring nodes layer by layer, where forward DP is similar in the sense that it starts from known states and propagates information forward to dependent states. It may help to see the similarities outlined in bullet form:
  + *BFS:*
    * Start from a node.
    * Visit all its immediate neighbors.
    * Next, visit the neighbors of those neighbors. And so on.
  + *Forward DP:*
    * Start with base states.
    * Use known solutions to base states to update the DP values of other states that depend on them.
    * Continue until all states are updated.

  Essentially, BFS explores a graph layer by layer, and forward DP updates states in a manner that follows from initial states outward to dependents.
- **Forward DP: "What can I do now?" vs. Backward DP: "How did I get here?":** These questions convey the underlying difference in perspective between forward and backward DP:
  + Backward DP: Think of where you came from to get to where you are.
      (In backward DP, we rely on known states to find the current state, which means we go *backwards* from the current state to the base cases to find a solution for the current state.)
  + Forward DP: Think of where you can go from where you currently are.
      (In forward DP, once we have a known state, we update other states that depend on it, which means we go *forward* from the base cases to the current state, which already has a solution thanks to the forward moving work.)

  The difference in approach/perspective can make some problems more intuitive and can also reveal different optimization opportunities.
- **Summary:** 

</details>

<details>
<summary> Why forward DP doesn't make sense for Fibonacci</summary>

Some problems naturally fit backward DP (e.g., problems where solutions can easily be expressed by a direct recurrence relation), and others more naturally fit forward DP (e.g., SSSP algorithms like Dijkstra's, where we use known shortest paths to update others).

<div align='center' className='centeredImageDiv'>
  <img width='600px' src={require('@site/static/img/templates/dp/f1.png').default} />
</div>

</details>